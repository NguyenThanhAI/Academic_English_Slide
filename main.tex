%% This Beamer template is based on the one found here: https://github.com/sanhacheong/stanford-beamer-presentation, and edited to be used for Stanford ARM Lab

\documentclass[10pt]{beamer}
%\mode<presentation>{}

\usepackage{media9}
\usepackage{amssymb,amsmath,amsthm,enumerate}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage[parfill]{parskip}
\usepackage{graphicx,animate}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{amsfonts,amscd}
\usepackage[]{units}
\usepackage{listings}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{physics}
\usepackage{movie15}
% Enable colored hyperlinks
\hypersetup{colorlinks=true}

% The following three lines are for crossmarks & checkmarks
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Numbered captions of tables, pictures, etc.
\setbeamertemplate{caption}[numbered]
\usepackage{media9} 
%\usepackage[superscript,biblabel]{cite}
%\usepackage{algorithmic}
%\usepackage{algorithm2e}
\usepackage{algpseudocode}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{caption}
%\usepackage{xcolor}
\usepackage{array}
%\renewcommand{\thealgocf}{}

% Bibliography settings
%\usepackage[style=authoryear]{biblatex}
%\setbeamertemplate{bibliography item}{\insertbiblabel}
%
%\addbibresource{bibliography.bib}
\usepackage[natbib,backend=biber,style=ieee, sorting=ynt]{biblatex}
\bibliography{ref.bib}

% Glossary entries
\usepackage[acronym]{glossaries}
\newacronym{ML}{ML}{machine learning}
\newacronym{HRI}{HRI}{human-robot interactions}
\newacronym{RNN}{RNN}{Recurrent Neural Network}
\newacronym{LSTM}{LSTM}{Long Short-Term Memory}


\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}

\newcommand{\empy}[1]{{\color{darkorange}\emph{#1}}}
\newcommand{\empr}[1]{{\color{cardinalred}\emph{#1}}}
\newcommand{\examplebox}[2]{
\begin{tcolorbox}[colframe=darkcardinal,colback=boxgray,title=#1]
#2
\end{tcolorbox}}

\usetheme{Stanford} 
\input{./style_files_stanford/my_beamer_defs.sty}
\logo{\includegraphics[height=0.5in]{logos/HUS-name.jpg}}

\makeatletter
\let\@@magyar@captionfix\relax
\makeatother

\title[Title]{Backpropagation in Training \\ Deep Neural Networks}


\begin{document}

\nocite{*}

\author[Author]{
	\begin{tabular}{c} 
	\Large
	Chi Thanh Nguyen \\
    \footnotesize \href{mailto:nguyenchithanh\_sdh21@hus.edu.vn}{nguyenchithanh\_sdh21@hus.edu.vn} \\
	\Large
    Duc Thinh Nguyen \\
     \footnotesize \href{mailto:nguyenducthinh\_sdh21@hus.edu.vn}{nguyenducthinh\_sdh21@hus.edu.vn}
\end{tabular}
\vspace{-4ex}}

\institute{
	\vskip 5pt
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.5\textwidth}
			\centering
			\includegraphics[height=0.75in]{logos/HUS-logo.jpg}
		\end{subfigure}%
		~ 
		\begin{subfigure}[t]{0.5\textwidth}
			\centering
			\includegraphics[height=0.75in]{logos/MIM-logo.png}
		\end{subfigure}
	\end{figure}
	\vskip 10pt	
	Vietnam National University \\
	Hanoi University of Science\\
	Faculty of Mathematics, Mechanics and Informatics
	\vskip 3pt
}

% \date{June 15, 2020}
%\date{\today}

\begin{noheadline}
\begin{frame} \maketitle \end{frame}
\end{noheadline}

\setbeamertemplate{itemize items}[default]
\setbeamertemplate{itemize subitem}[circle]

\begin{frame}{Content}

\begin{enumerate}
	\item Introduction
 	\item Notations
	\item Formulations
	\item Example  
\end{enumerate}

\end{frame}

\begin{frame}{Introduction}

\end{frame}

\begin{frame}[allowframebreaks]{Notations}
	\begin{itemize}
		\item $a$: A scalar
  		\item $\bold{a}$: A vector
    	\item $\bold{A}$: A matrix
     	\item $\bold{A}_{i,:}$: Row $i$ of matrix $\bold{A}$
      	\item $\bold{A}_{:,j}$: Column $j$ of matrix $\bold{A}$
       	\item $\bold{W^{(l)}}$: weights connect from (l-1)-th layer to l-th layer
        \item $\bold{b}^{(l)}$: bias connect from (l-1)-th layer to l-th layer
        \item $w_{ij}^{(l)}$: weight connects from i-th neuron of (l-1)-th layer to jth neuron lth layer
        \item $z_{i}^{(l)}$: pre-activated output of neuron i-th of l-th layer
        \item $a_{i}^{(l)}$: activated output of neuron i-th of l-th layer
        \item $d^{(l)}$: number of neuron (dimension) of l-th layer
        \item $\bold{z}^{(l)}$: pre-activated output vector of l-th layer. $\bold{z}^{(l)} = \begin{bmatrix} z_1^{(l)} & z_2^{(l)} & \dots & z_{d^{(l)}}^{(l)}\end{bmatrix}^T$
        \item $\bold{a}^{(l)}$: activated output vector of l-th layer. $\bold{a}^{(l)} = \begin{bmatrix} a_1^{(l)} & a_2^{(l)} &\dots & a_{d^{(l)}}^{(l)}\end{bmatrix}^T$
        \item $\mathcal{L}$: loss function
        \item $\odot$: Element-wise product of two vectors
        \item $\bold{x}=\bold{a}^{(0)}$: input of neural network
	\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Formulations}
	\begin{itemize}
		\item $\bold{W}^{(l)}$: weights of l-th layer
    	\begin{equation*}
			\bold{W}^{(l)}=\begin{bmatrix} w_{11}^{(l)} & w_{12}^{(l)} & w_{13}^{(l)} & \dots & w_{1d^{(l)}}^{(l)} \\ w_{21}^{(l)} & w_{22}^{(l)} & w_{23}^{(l)} & \dots & w_{2d^{(l)}}^{(l)} \\ \vdots & \thickspace & \thickspace & \thickspace & \vdots \\ w_{d^{(l-1)}1}^{(l)} & w_{d^{(l-1)}2}^{(l)} & w_{d^{(l-1)}3}^{(l)} & \dots & w_{d^{(l-1)}d^{(l)}}^{(l)}\end{bmatrix} \in \mathbb{R}^{d^{(l-1)} \times d^{(l)}}
		\end{equation*}
		\item $\bold{b}^{(l)}$: bias of l-th layer
		\begin{equation*}
			\bold{b}^{(l)}=\begin{bmatrix} b_1^{(l)} \\  b_2^{(l)} \\ \vdots \\ b_{d^{(l)}}^{(l)} \end{bmatrix}
		\end{equation*}
		\item Pre-activated output of neuron i-th of l-th layer:
		\begin{equation}
			z_j^{(l)}=\sum_{i=1}^{d^{(l-1)}}w_{ij}^{(l)}a_i^{(l-1)}+b_j^{(l)}=\Big(\bold{W}_{:,j}\Big)^T \bold{a}^{(l-1)} + b_j^{(l)}
		\end{equation}
		\item Activated output of neuron i-th of l-th layer:
		\begin{equation}
			a_j^{(l)} = \bold{f}(z_j^{(l)})
		\end{equation}
		\item Pre-activated output in matrix form:
		\begin{equation}
			\bold{z}^{(l)}=\Big(\bold{W}^{(l)}\Big)^T\bold{a}^{(l-1)} + \bold{b}^{(l)}, l=1, \dots, L
		\end{equation}
		\item Activated output in matrix form:
		\begin{equation}
			\bold{a}^{(l)}=\bold{f}(\bold{z}^{(l)}), l=1, \dots, L
		\end{equation}
		\item Loss function:
		\begin{equation}
			\mathcal{L} = \bold{g}\Big(\bold{a}^{(L)} \Big)
		\end{equation}
	\end{itemize}

	We need to calculate partial derivatives of $\mathcal{L}$ with respect to all $\bold{W}^{(l)}$ and $\bold{b}^{(l)}, l=1, \dots, L$

	\begin{equation}
		\dfrac{\partial \mathcal{L}}{w_{ij}^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(l)}}\dfrac{\partial z_j^{(l)}}{w_{ij}^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(l)}}a_i^{(l-1)}
	\end{equation}

	\begin{equation}
		\dfrac{\partial \mathcal{L}}{\partial b_j^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(l)}}\dfrac{\partial z_j^{(l)}}{\partial b_j^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(l)}}
	\end{equation}

	We set:
	\begin{equation}
		\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(l)}}:=e_j^{(l)}
	\end{equation}

	At output layer ($L$-th layer):

	\begin{equation}
		\begin{aligned}
			\dfrac{\partial \mathcal{L}}{w_{ij}^{(L)}}&=\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(L)}}\dfrac{\partial z_j^{(L)}}{w_{ij}^{(L)}}\\&=\dfrac{\partial \mathcal{L}}{\partial z_{j}^{(L)}}a_i^{(L-1)}\\&=\dfrac{\partial\mathcal{L}}{\partial a_j^{(L)}}\dfrac{\partial a_j^{(L)}}{\partial z_j^{(L)}}a_i^{(L-1)}\\&=\dfrac{\partial\mathcal{L}}{\partial a_j^{(L)}}\bold{f}^{\prime}\Big(z_j^{(L)}\Big)a_i^{(L-1)}
		\end{aligned}
	\end{equation}

	\begin{equation}
		\begin{aligned}
			\bold{e}^{(L)}&=\begin{bmatrix} \dfrac{\partial\mathcal{L}}{\partial z_1^{(L)}} & \dfrac{\partial\mathcal{L}}{\partial z_2^{(L)}} & \dots & \dfrac{\partial\mathcal{L}}{\partial z_{d^{(L)}}^{(L)}} \end{bmatrix}^T\\&=\begin{bmatrix} \dfrac{\partial\mathcal{L}}{\partial a_1^{(L)}}\dfrac{\partial a_1^{(L)}}{\partial z_1^{(L)}} & \dfrac{\partial\mathcal{L}}{\partial a_2^{(L)}}\dfrac{\partial a_2^{(L)}}{\partial z_2^{(L)}} & \dots & \dfrac{\partial\mathcal{L}}{\partial a_{d^{(L)}}^{(L)}}\dfrac{\partial a_{d^{(L)}}^{(L)}}{\partial z_{d^{(L)}}^{(L)}} \end{bmatrix}^T\\&=\begin{bmatrix} \dfrac{\partial\mathcal{L}}{\partial a_1^{(L)}}\bold{f}^{\prime}\Big(z_1^{(L)}\Big) & \dfrac{\partial\mathcal{L}}{\partial a_2^{(L)}}\bold{f}^{\prime}\Big(z_2^{(L)}\Big) & \dots & \dfrac{\partial\mathcal{L}}{\partial a_{d^{(L)}}^{(L)}}\bold{f}^{\prime}\Big(z_{d^{(L)}}^{(L)}\Big) \end{bmatrix}^T\\&=\dfrac{\partial \mathcal{L}}{\partial \bold{a}^{(L)}} \odot \bold{f}^{\prime}(\bold{z}^{(L)})
		\end{aligned}
	\end{equation}

	\begin{equation}
		\Rightarrow \begin{cases}\dfrac{\partial \mathcal{L}}{\partial \bold{W}^{(L)}}=\bold{a}^{(L-1)}\Big(\bold{e}^{(L)}\Big)^T \\ \dfrac{\partial \mathcal{L}}{\partial \bold{b}^{(L)}}=\bold{e}^{(L)} \end{cases}
	\end{equation}

	At l-th layer ($1 \leq l < L$):

	\begin{equation}
		\dfrac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_j^{(l)}}\dfrac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_j^{(l)}} a_i^{(l-1)}=e_j^{(l)}a_i^{(l-1)}
	\end{equation}

	\begin{equation}
		\dfrac{\partial \mathcal{L}}{\partial b_j^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_j^{(l)}}\dfrac{\partial z_j^{(l)}}{\partial b_j^{(l)}}=\dfrac{\partial \mathcal{L}}{\partial z_j^{(l)}} = e_j^{(l)}
	\end{equation}

	\begin{equation}
		\Rightarrow \begin{cases} \dfrac{\partial \mathcal{L}}{\partial \bold{W}^{(l)}}=\bold{a}^{(l-1)}\Big(\bold{e}^{(l)}\Big)^T \\ \dfrac{\partial \mathcal{L}}{\partial \bold{b}^{(l)}}=\bold{e}^{(l)} \end{cases}
	\end{equation}

	We consider $\mathcal{L} = \mathcal{L}\Big(\bold{z}^{(l+1)}\Big)=\mathcal{L}\Big(z_1^{(l+1)}, z_2^{(l+1)}, \dots, z_{d^{(l+1)}}^{(l+1)}\Big)$:
	\begin{equation}
		\begin{aligned}
			\dfrac{\partial \mathcal{L}}{\partial z_j^{(l)}}&=\sum_{k=1}^{d^{(l+1)}}\dfrac{\partial \mathcal{L}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}}\\&=\sum_{k=1}^{d^{(l+1)}} \dfrac{\partial \mathcal{L}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_j^{(l)}} \dfrac{\partial a_j^{(l)}}{\partial z_j^{(l)}}\\&=\sum_{k=1}^{d^{(l+1)}}\dfrac{\partial \mathcal{L}}{\partial z_k^{(l+1)}}w_{jk}^{(l+1)}\bold{f}^{\prime}\Big(z_j^{(l)}\Big)
		\end{aligned}
	\end{equation}

	\begin{equation}
		\begin{aligned}
			e_j^{(l)}&=\dfrac{\partial \mathcal{L}}{\partial z_j^{(l)}}\\&=\sum_{k=1}^{d^{(l+1)}}e_k^{(l+1)}w_{jk}^{(l+1)}\bold{f}^{\prime}\Big(z_j^{(l)}\Big)\\&=\bold{f}^{\prime}(z_j^{(l)})\bold{W}_{j,:}^{(l+1)}\bold{e}^{(l+1)}
		\end{aligned}
	\end{equation}

	In matrix form:

	\begin{equation}
		\bold{e}^{(l)}=\bold{W}^{(l+1)}\bold{e}^{(l+1)}\odot\bold{f}^{\prime}(\bold{z}^{(l)})
	\end{equation}

	\framebreak

	\begingroup
	\small
	\scalebox{0.6}{
	\begin{minipage}{1.2 \linewidth}
	\begin{algorithm}[H]
		\caption{Backpropagation with single training sample}
		\begin{algorithmic}[1]
			\For{$l=1,\dots, L$}
				\State{Initialize $\bold{W}^{(l)}$ and $\bold{b}^{(l)}$ randomly}
			\EndFor
			\While{not converged}
				\State {Get training sample $\bold{x}$}
				\State{$\bold{a}^{(0)}=\bold{x}$}
				\For{$l=1,\dots, L$} \Comment{Compute forward pass}
					\State{$\bold{z}^{(l)}=\Big(\bold{W}^{(l)}\Big)^T \bold{a}^{(l-1)} + \bold{b}^{(l)}$}
					\State{$\bold{a}^{(l)}=\bold{f}\Big(\bold{z}^{(l)}\Big)$}
				\EndFor
				\State{Compute $\mathcal{L}=\bold{g}\Big(\bold{a}^{(L)}\Big)$}
				\State{$\bold{e}^{(L)}=\dfrac{\partial \mathcal{L}}{\partial \bold{a}^{(L)}} \odot \bold{f}^{\prime}(\bold{z}^{(L)})$}
				\State{$\begin{cases} \dfrac{\partial \mathcal{L}}{\partial \bold{W}^{(L)}}=\bold{a}^{(L-1)}\Big(\bold{e}^{(L)}\Big)^T \\ \dfrac{\partial \mathcal{L}}{\partial \bold{b}^{(L)}}=\bold{e}^{(L)} \end{cases}$}
				\For{$l=L-1, \dots, 1$} \Comment{Compute backpropagation (backward pass)}
					\State{$\bold{e}^{(l)}=\bold{W}^{(l+1)}\bold{e}^{(l+1)}\odot\bold{f}^{\prime}(\bold{z}^{(l)})$}
					\State{$\begin{cases} \dfrac{\partial \mathcal{L}}{\partial \bold{W}^{(l)}}=\bold{a}^{(l-1)}\Big(\bold{e}^{(l)}\Big)^T \\ \dfrac{\partial \mathcal{L}}{\partial \bold{b}^{(l)}}=\bold{e}^{(l)} \end{cases}$}
				\EndFor
				\For{$l=1, \dots, L$} \Comment{Update neural network weights}
					\State{$\begin{cases} \bold{W}^{(l)} \leftarrow \bold{W}^{(l)} - \alpha \dfrac{\partial \mathcal{L}}{\partial \bold{W}^{(l)}} \\ \bold{b}^{(l)} \leftarrow \bold{b}^{(l)} - \alpha \dfrac{\partial \mathcal{L}}{\partial \bold{b}^{(l)}} \end{cases}$}
				\EndFor
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	\end{minipage}
	}
	\endgroup


\end{frame}

\begin{frame}{Discussion}
\end{frame}

\begin{frame}{Conclusion}
\end{frame}

\begin{frame}{Acknowledgements}
\end{frame}

\begin{frame}[allowframebreaks]{References}
\printbibliography
\end{frame}

\end{document}